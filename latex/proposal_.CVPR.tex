\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Google Landmark Recognition Challenge}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}


%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\section{Abstract}

After receiving paper reviews, authors may optionally submit a rebuttal to address the reviewers' comments, which will be limited to a {\bf one page} PDF file.  Please follow the steps and style guidelines outlined below for submitting your author response.

Note that the author rebuttal is optional and, following similar guidelines to previous CVPR conferences, it is meant to provide you with an opportunity to rebut factual errors or to supply additional information requested by the reviewers. It is NOT intended to add new contributions (theorems, algorithms, experiments) that were not included in the original submission and were not requested by the reviewers. You may optionally add a figure, graph or proof to your rebuttal to better illustrate your answer to the reviewers' comments.

The rebuttal must adhere to the same blind-submission as the original submission and must comply with this rebuttal-formatted template.

%-------------------------------------------------------------------------

\section{Instroduction}
Author responses must be no longer than 1 page in length including any references and figures.  Overlength responses will simply not be reviewed.  This includes responses where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.  Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.

%------------------------------------------------------------------------
\section{Related Work}

With a drastic boom in the deep learning field, this group of algorithms has considerably improved the state of the art in image classification. Some Convolutional Neural Network Architectures can achieve fairly high accuracy on a complicated image dataset. ResNet, for example, successfully obtained a Top-5 classification accuracy of 96.53\% on ImageNet. However, compared with traditional image classification with the ImageNet dataset, this Google Landmark Recognition Challenge has some contrasting differences. The number of classes in the Landmark Change is 20 times bigger than the class size of ImageNet, but the size of training examples per class in the given "Google Landmarks" dataset might be quite small. To address these problems, the previous competitors in this challenge tried different combinations of CNN architectures including ResNet50, Inception-ResNet, and SEResNet. The highest accuracy achieved in this year's challenge is about 0.37 in Kaggle Leader-Board score which again indicates a high difficulty level of this classification task. 

%-------------------------------------------------------------------------
\section{Technical overview}



%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{1.8in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}

%-------------------------------------------------------------------------
\subsection{Data Cleaning}

The dataset, Google-Landmarks-v1, provided by Google, contains 5 million images of more than 200,000 different landmarks. The images were collected from photographers around the world who labeled their photos and supplemented them with historical and lesser-known images from Wikimedia Commons.

For the data cleaning stage, we are considering adopting similar strategies mentioned by Ozaki et al. The first step is to remove all classes with no more than 3 training samples (53,435 classes in total). Then by applying spatial verification to the filtered images by k nearest neighbor search, we expect the cleaned dataset contains around 2 million images with roughly 100,000 labels.

\subsection{Modeling}
Considering using ResNet-101 and SE-ResNeXt-101 as backbones trained with cosine-softmax / softmax based losses.

%When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the  figure width as a multiple of the line width as in the example below
%{\small\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.eps}
%\end{verbatim}
%}

%
%{\small
%\bibliographystyle{ieee}
%\bibliography{egbib}
%}

\section{Expected Outcome}
After applying the data cleaning method, the training data should not contain classes with the size of the training samples smaller than three. Visually unrelated images within the same class should also be discarded. After the meticulous construction of models and tuning of the parameters, we expect our model to achieve around 0.25 in the Kaggle leaderboard score.  

\begin{thebibliography}{9}
\bibitem{latexcompanion} 

 
%\bibitem{einstein} 
%Albert Einstein. 
%\textit{Zur Elektrodynamik bewegter K{\"o}rper}. (German) 
%[\textit{On the electrodynamics of moving bodies}]. 
%Annalen der Physik, 322(10):891?921, 1905.
% 
%\bibitem{knuthwebsite} 
%Knuth: Computers and Typesetting,
%\\\texttt{http://www-cs-faculty.stanford.edu/\~{}uno/abcde.html}
\end{thebibliography}

\end{document}
